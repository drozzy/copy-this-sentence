{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loaders_transformer import train_iterator, test_iterator, valid_iterator\n",
    "from bleu import bleu_ignore_eos\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchtext\n",
    "from torchtext.datasets import TranslationDataset, Multi30k\n",
    "from torchtext.data import Field, BucketIterator\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_loader, vocab = train_iterator(device=device, batch_size=32)\n",
    "test_iterator, vocab_test = test_iterator(device=device, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, kernel_size, dropout, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert kernel_size % 2 == 1, \"Kernel size must be odd!\"\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.dropout = dropout\n",
    "        self.device = device\n",
    "        \n",
    "        self.scale = torch.sqrt(torch.FloatTensor([0.5])).to(device)\n",
    "        \n",
    "        self.tok_embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.pos_embedding = nn.Embedding(100, emb_dim)\n",
    "        \n",
    "        self.emb2hid = nn.Linear(emb_dim, hid_dim)\n",
    "        self.hid2emb = nn.Linear(hid_dim, emb_dim)\n",
    "        \n",
    "        self.convs = nn.ModuleList([nn.Conv1d(in_channels = hid_dim, \n",
    "                                              out_channels = 2 * hid_dim, \n",
    "                                              kernel_size = kernel_size, \n",
    "                                              padding = (kernel_size - 1) // 2)\n",
    "                                    for _ in range(n_layers)])\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        \n",
    "        #src = [batch size, src sent len]\n",
    "        \n",
    "        #create position tensor\n",
    "        pos = torch.arange(0, src.shape[1]).unsqueeze(0).repeat(src.shape[0], 1).to(self.device)\n",
    "        \n",
    "        #pos = [batch size, src sent len]\n",
    "        \n",
    "        #embed tokens and positions\n",
    "        tok_embedded = self.tok_embedding(src)\n",
    "        pos_embedded = self.pos_embedding(pos)\n",
    "        \n",
    "        #tok_embedded = pos_embedded = [batch size, src sent len, emb dim]\n",
    "        \n",
    "        #combine embeddings by elementwise summing\n",
    "        embedded = self.dropout(tok_embedded + pos_embedded)\n",
    "        \n",
    "        #embedded = [batch size, src sent len, emb dim]\n",
    "        \n",
    "        #pass embedded through linear layer to go through emb dim -> hid dim\n",
    "        conv_input = self.emb2hid(embedded)\n",
    "        \n",
    "        #conv_input = [batch size, src sent len, hid dim]\n",
    "        \n",
    "        #permute for convolutional layer\n",
    "        conv_input = conv_input.permute(0, 2, 1) \n",
    "        \n",
    "        #conv_input = [batch size, hid dim, src sent len]\n",
    "        \n",
    "        for i, conv in enumerate(self.convs):\n",
    "        \n",
    "            #pass through convolutional layer\n",
    "            conved = conv(self.dropout(conv_input))\n",
    "\n",
    "            #conved = [batch size, 2*hid dim, src sent len]\n",
    "\n",
    "            #pass through GLU activation function\n",
    "            conved = F.glu(conved, dim = 1)\n",
    "\n",
    "            #conved = [batch size, hid dim, src sent len]\n",
    "            \n",
    "            #apply residual connection\n",
    "            conved = (conved + conv_input) * self.scale\n",
    "\n",
    "            #conved = [batch size, hid dim, src sent len]\n",
    "            \n",
    "            #set conv_input to conved for next loop iteration\n",
    "            conv_input = conved\n",
    "        \n",
    "        #permute and convert back to emb dim\n",
    "        conved = self.hid2emb(conved.permute(0, 2, 1))\n",
    "        \n",
    "        #conved = [batch size, src sent len, emb dim]\n",
    "        \n",
    "        #elementwise sum output (conved) and input (embedded) to be used for attention\n",
    "        combined = (conved + embedded) * self.scale\n",
    "        \n",
    "        #combined = [batch size, src sent len, emb dim]\n",
    "        \n",
    "        return conved, combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, kernel_size, dropout, pad_idx, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.dropout = dropout\n",
    "        self.pad_idx = pad_idx\n",
    "        self.device = device\n",
    "        \n",
    "        self.scale = torch.sqrt(torch.FloatTensor([0.5])).to(device)\n",
    "        \n",
    "        self.tok_embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.pos_embedding = nn.Embedding(100, emb_dim)\n",
    "        \n",
    "        self.emb2hid = nn.Linear(emb_dim, hid_dim)\n",
    "        self.hid2emb = nn.Linear(hid_dim, emb_dim)\n",
    "        \n",
    "        self.attn_hid2emb = nn.Linear(hid_dim, emb_dim)\n",
    "        self.attn_emb2hid = nn.Linear(emb_dim, hid_dim)\n",
    "        \n",
    "        self.out = nn.Linear(emb_dim, output_dim)\n",
    "        \n",
    "        self.convs = nn.ModuleList([nn.Conv1d(hid_dim, 2*hid_dim, kernel_size)\n",
    "                                    for _ in range(n_layers)])\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "      \n",
    "    def calculate_attention(self, embedded, conved, encoder_conved, encoder_combined):\n",
    "        \n",
    "        #embedded = [batch size, trg sent len, emb dim]\n",
    "        #conved = [batch size, hid dim, trg sent len]\n",
    "        #encoder_conved = encoder_combined = [batch size, src sent len, emb dim]\n",
    "        \n",
    "        #permute and convert back to emb dim\n",
    "        conved_emb = self.attn_hid2emb(conved.permute(0, 2, 1))\n",
    "        \n",
    "        #conved_emb = [batch size, trg sent len, emb dim]\n",
    "        \n",
    "        combined = (embedded + conved_emb) * self.scale\n",
    "        \n",
    "        #combined = [batch size, trg sent len, emb dim]\n",
    "                \n",
    "        energy = torch.matmul(combined, encoder_conved.permute(0, 2, 1))\n",
    "        \n",
    "        #energy = [batch size, trg sent len, src sent len]\n",
    "        \n",
    "        attention = F.softmax(energy, dim=2)\n",
    "        \n",
    "        #attention = [batch size, trg sent len, src sent len]\n",
    "            \n",
    "        attended_encoding = torch.matmul(attention, (encoder_conved + encoder_combined))\n",
    "        \n",
    "        #attended_encoding = [batch size, trg sent len, emd dim]\n",
    "        \n",
    "        #convert from emb dim -> hid dim\n",
    "        attended_encoding = self.attn_emb2hid(attended_encoding)\n",
    "        \n",
    "        #attended_encoding = [batch size, trg sent len, hid dim]\n",
    "        \n",
    "        attended_combined = (conved + attended_encoding.permute(0, 2, 1)) * self.scale\n",
    "        \n",
    "        #attended_combined = [batch size, hid dim, trg sent len]\n",
    "        \n",
    "        return attention, attended_combined\n",
    "        \n",
    "    def forward(self, trg, encoder_conved, encoder_combined):\n",
    "        \n",
    "        #trg = [batch size, trg sent len]\n",
    "        #encoder_conved = encoder_combined = [batch size, src sent len, emb dim]\n",
    "                \n",
    "        #create position tensor\n",
    "        pos = torch.arange(0, trg.shape[1]).unsqueeze(0).repeat(trg.shape[0], 1).to(device)\n",
    "        \n",
    "        #pos = [batch size, trg sent len]\n",
    "        \n",
    "        #embed tokens and positions\n",
    "        tok_embedded = self.tok_embedding(trg)\n",
    "        pos_embedded = self.pos_embedding(pos)\n",
    "        \n",
    "        #tok_embedded = [batch size, trg sent len, emb dim]\n",
    "        #pos_embedded = [batch size, trg sent len, emb dim]\n",
    "        \n",
    "        #combine embeddings by elementwise summing\n",
    "        embedded = self.dropout(tok_embedded + pos_embedded)\n",
    "        \n",
    "        #embedded = [batch size, trg sent len, emb dim]\n",
    "        \n",
    "        #pass embedded through linear layer to go through emb dim -> hid dim\n",
    "        conv_input = self.emb2hid(embedded)\n",
    "        \n",
    "        #conv_input = [batch size, trg sent len, hid dim]\n",
    "        \n",
    "        #permute for convolutional layer\n",
    "        conv_input = conv_input.permute(0, 2, 1) \n",
    "        \n",
    "        #conv_input = [batch size, hid dim, trg sent len]\n",
    "        \n",
    "        for i, conv in enumerate(self.convs):\n",
    "        \n",
    "            #apply dropout\n",
    "            conv_input = self.dropout(conv_input)\n",
    "        \n",
    "            #need to pad so decoder can't \"cheat\"\n",
    "            padding = torch.zeros(conv_input.shape[0], conv_input.shape[1], self.kernel_size-1).fill_(self.pad_idx).to(device)\n",
    "            padded_conv_input = torch.cat((padding, conv_input), dim=2)\n",
    "        \n",
    "            #padded_conv_input = [batch size, hid dim, trg sent len + kernel size - 1]\n",
    "        \n",
    "            #pass through convolutional layer\n",
    "            conved = conv(padded_conv_input)\n",
    "\n",
    "            #conved = [batch size, 2*hid dim, trg sent len]\n",
    "            \n",
    "            #pass through GLU activation function\n",
    "            conved = F.glu(conved, dim=1)\n",
    "\n",
    "            #conved = [batch size, hid dim, trg sent len]\n",
    "            \n",
    "            attention, conved = self.calculate_attention(embedded, conved, encoder_conved, encoder_combined)\n",
    "            \n",
    "            #attention = [batch size, trg sent len, src sent len]\n",
    "            #conved = [batch size, hid dim, trg sent len]\n",
    "            \n",
    "            #apply residual connection\n",
    "            conved = (conved + conv_input) * self.scale\n",
    "            \n",
    "            #conved = [batch size, hid dim, trg sent len]\n",
    "            \n",
    "            #set conv_input to conved for next loop iteration\n",
    "            conv_input = conved\n",
    "            \n",
    "        conved = self.hid2emb(conved.permute(0, 2, 1))\n",
    "         \n",
    "        #conved = [batch size, trg sent len, hid dim]\n",
    "            \n",
    "        output = self.out(self.dropout(conved))\n",
    "        \n",
    "        #output = [batch size, trg sent len, output dim]\n",
    "            \n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, src, trg):\n",
    "        \n",
    "        #src = [batch size, src sent len]\n",
    "        #trg = [batch size, trg sent len]\n",
    "           \n",
    "        #calculate z^u (encoder_conved) and e (encoder_combined)\n",
    "        #encoder_conved is output from final encoder conv. block\n",
    "        #encoder_combined is encoder_conved plus (elementwise) src embedding plus positional embeddings \n",
    "        encoder_conved, encoder_combined = self.encoder(src)\n",
    "            \n",
    "        #encoder_conved = [batch size, src sent len, emb dim]\n",
    "        #encoder_combined = [batch size, src sent len, emb dim]\n",
    "        \n",
    "        #calculate predictions of next words\n",
    "        #output is a batch of predictions for each word in the trg sentence\n",
    "        #attention a batch of attention scores across the src sentence for each word in the trg sentence\n",
    "        output, attention = self.decoder(trg, encoder_conved, encoder_combined)\n",
    "        \n",
    "        #output = [batch size, trg sent len, output dim]\n",
    "        #attention = [batch size, trg sent len, src sent len]\n",
    "        \n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(vocab)\n",
    "OUTPUT_DIM = len(vocab)\n",
    "EMB_DIM = 256\n",
    "HID_DIM = 512\n",
    "ENC_LAYERS = 10\n",
    "DEC_LAYERS = 10\n",
    "ENC_KERNEL_SIZE = 3\n",
    "DEC_KERNEL_SIZE = 3\n",
    "ENC_DROPOUT = 0.25\n",
    "DEC_DROPOUT = 0.25\n",
    "PAD_IDX = vocab.stoi['<pad>']\n",
    "EOS_TOKEN = vocab.stoi['<eos>']\n",
    "    \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "enc = Encoder(INPUT_DIM, EMB_DIM, HID_DIM, ENC_LAYERS, ENC_KERNEL_SIZE, ENC_DROPOUT, device)\n",
    "dec = Decoder(OUTPUT_DIM, EMB_DIM, HID_DIM, DEC_LAYERS, DEC_KERNEL_SIZE, DEC_DROPOUT, PAD_IDX, device)\n",
    "\n",
    "model = Seq2Seq(enc, dec, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 49,320,286 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        src = batch.sentence\n",
    "        trg = batch.sentence\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output, _ = model(src, trg[:,1:])\n",
    "        \n",
    "        #output = [batch size, trg sent len - 1, output dim]\n",
    "        #trg = [batch size, trg sent len]\n",
    "        \n",
    "        output = output.contiguous().view(-1, output.shape[-1])\n",
    "        trg = trg[:,1:].contiguous().view(-1)\n",
    "        \n",
    "        #output = [batch size * trg sent len - 1, output dim]\n",
    "        #trg = [batch size * trg sent len - 1]\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    avg_bleu = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            src = batch.sentence\n",
    "            trg = batch.sentence\n",
    "\n",
    "            output, _ = model(src, trg[:,1:])\n",
    "            \n",
    "            avg_bleu += compute_bleu(output, trg[:,1:])\n",
    "        \n",
    "            #output = [batch size, trg sent len - 1, output dim]\n",
    "            #trg = [batch size, trg sent len]\n",
    "\n",
    "            output = output.contiguous().view(-1, output.shape[-1])\n",
    "            trg = trg[:,1:].contiguous().view(-1)\n",
    "\n",
    "            #output = [batch size * trg sent len - 1, output dim]\n",
    "            #trg = [batch size * trg sent len - 1]\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), avg_bleu / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bleu(hypothesis, reference):\n",
    "    hypothesis = hypothesis.argmax(dim=2)\n",
    "    \n",
    "    hypothesis = hypothesis.detach().cpu().numpy()\n",
    "    reference = reference.detach().cpu().numpy()\n",
    "    scores = []\n",
    "    for i in range(reference.shape[0]):\n",
    "        a = reference[i]\n",
    "        b = hypothesis[i]\n",
    "        score = float(bleu_ignore_eos(reference=a.tolist(), hypothesis=b.tolist(), eos_token=EOS_TOKEN))\n",
    "        scores.append(score)\n",
    "    \n",
    "    return torch.tensor(scores).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 0m 32s\n",
      "\tTrain Loss: 9.652 | Train PPL: 15548.802\n",
      "\t Val. Loss: 3.772 |  Val. PPL:  43.471 | Avg Bleu: 0.136464\n",
      "Epoch: 02 | Time: 0m 31s\n",
      "\tTrain Loss: 3.449 | Train PPL:  31.469\n",
      "\t Val. Loss: 2.306 |  Val. PPL:  10.038 | Avg Bleu: 0.399130\n",
      "Epoch: 03 | Time: 0m 31s\n",
      "\tTrain Loss: 2.418 | Train PPL:  11.229\n",
      "\t Val. Loss: 1.776 |  Val. PPL:   5.909 | Avg Bleu: 0.563227\n",
      "Epoch: 04 | Time: 0m 30s\n",
      "\tTrain Loss: 1.835 | Train PPL:   6.263\n",
      "\t Val. Loss: 1.570 |  Val. PPL:   4.809 | Avg Bleu: 0.632465\n",
      "Epoch: 05 | Time: 0m 30s\n",
      "\tTrain Loss: 1.441 | Train PPL:   4.225\n",
      "\t Val. Loss: 1.468 |  Val. PPL:   4.338 | Avg Bleu: 0.674440\n",
      "Epoch: 06 | Time: 0m 31s\n",
      "\tTrain Loss: 1.151 | Train PPL:   3.162\n",
      "\t Val. Loss: 1.377 |  Val. PPL:   3.964 | Avg Bleu: 0.698035\n",
      "Epoch: 07 | Time: 0m 31s\n",
      "\tTrain Loss: 0.919 | Train PPL:   2.506\n",
      "\t Val. Loss: 1.352 |  Val. PPL:   3.864 | Avg Bleu: 0.711923\n",
      "Epoch: 08 | Time: 0m 31s\n",
      "\tTrain Loss: 0.728 | Train PPL:   2.071\n",
      "\t Val. Loss: 1.363 |  Val. PPL:   3.907 | Avg Bleu: 0.722625\n",
      "Epoch: 09 | Time: 0m 31s\n",
      "\tTrain Loss: 0.561 | Train PPL:   1.752\n",
      "\t Val. Loss: 1.314 |  Val. PPL:   3.722 | Avg Bleu: 0.727715\n",
      "Epoch: 10 | Time: 0m 32s\n",
      "\tTrain Loss: 0.432 | Train PPL:   1.540\n",
      "\t Val. Loss: 1.438 |  Val. PPL:   4.211 | Avg Bleu: 0.730485\n",
      "Epoch: 11 | Time: 0m 31s\n",
      "\tTrain Loss: 0.333 | Train PPL:   1.395\n",
      "\t Val. Loss: 1.477 |  Val. PPL:   4.381 | Avg Bleu: 0.732290\n",
      "Epoch: 12 | Time: 0m 31s\n",
      "\tTrain Loss: 0.266 | Train PPL:   1.305\n",
      "\t Val. Loss: 1.483 |  Val. PPL:   4.406 | Avg Bleu: 0.738456\n",
      "Epoch: 13 | Time: 0m 31s\n",
      "\tTrain Loss: 0.221 | Train PPL:   1.247\n",
      "\t Val. Loss: 1.543 |  Val. PPL:   4.679 | Avg Bleu: 0.735772\n",
      "Epoch: 14 | Time: 0m 30s\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.204\n",
      "\t Val. Loss: 1.580 |  Val. PPL:   4.856 | Avg Bleu: 0.740903\n",
      "Epoch: 15 | Time: 0m 30s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.180\n",
      "\t Val. Loss: 1.526 |  Val. PPL:   4.600 | Avg Bleu: 0.745340\n",
      "Epoch: 16 | Time: 0m 30s\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.153\n",
      "\t Val. Loss: 1.591 |  Val. PPL:   4.908 | Avg Bleu: 0.747363\n",
      "Epoch: 17 | Time: 0m 33s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.137\n",
      "\t Val. Loss: 1.610 |  Val. PPL:   5.005 | Avg Bleu: 0.748993\n",
      "Epoch: 18 | Time: 0m 31s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 1.563 |  Val. PPL:   4.774 | Avg Bleu: 0.752366\n",
      "Epoch: 19 | Time: 0m 30s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 1.666 |  Val. PPL:   5.293 | Avg Bleu: 0.754406\n",
      "Epoch: 20 | Time: 0m 31s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 1.615 |  Val. PPL:   5.026 | Avg Bleu: 0.760632\n",
      "Epoch: 21 | Time: 0m 31s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 1.637 |  Val. PPL:   5.137 | Avg Bleu: 0.766228\n",
      "Epoch: 22 | Time: 0m 31s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 1.658 |  Val. PPL:   5.248 | Avg Bleu: 0.770104\n",
      "Epoch: 23 | Time: 0m 31s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 1.626 |  Val. PPL:   5.082 | Avg Bleu: 0.774563\n",
      "Epoch: 24 | Time: 0m 31s\n",
      "\tTrain Loss: 0.076 | Train PPL:   1.079\n",
      "\t Val. Loss: 1.701 |  Val. PPL:   5.479 | Avg Bleu: 0.775314\n",
      "Epoch: 25 | Time: 0m 31s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.072\n",
      "\t Val. Loss: 1.649 |  Val. PPL:   5.203 | Avg Bleu: 0.780772\n",
      "Epoch: 26 | Time: 0m 31s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 1.718 |  Val. PPL:   5.574 | Avg Bleu: 0.779806\n",
      "Epoch: 27 | Time: 0m 31s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.067\n",
      "\t Val. Loss: 1.916 |  Val. PPL:   6.796 | Avg Bleu: 0.791219\n",
      "Epoch: 28 | Time: 0m 31s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 1.765 |  Val. PPL:   5.842 | Avg Bleu: 0.793668\n",
      "Epoch: 29 | Time: 0m 31s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 1.846 |  Val. PPL:   6.333 | Avg Bleu: 0.794538\n",
      "Epoch: 30 | Time: 0m 31s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 1.887 |  Val. PPL:   6.599 | Avg Bleu: 0.802438\n",
      "Epoch: 31 | Time: 0m 30s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 1.927 |  Val. PPL:   6.869 | Avg Bleu: 0.801133\n",
      "Epoch: 32 | Time: 0m 30s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 1.918 |  Val. PPL:   6.810 | Avg Bleu: 0.804348\n",
      "Epoch: 33 | Time: 0m 30s\n",
      "\tTrain Loss: 0.051 | Train PPL:   1.052\n",
      "\t Val. Loss: 1.848 |  Val. PPL:   6.345 | Avg Bleu: 0.809401\n",
      "Epoch: 34 | Time: 0m 31s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 1.868 |  Val. PPL:   6.478 | Avg Bleu: 0.810523\n",
      "Epoch: 35 | Time: 0m 31s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 1.861 |  Val. PPL:   6.428 | Avg Bleu: 0.815766\n",
      "Epoch: 36 | Time: 0m 32s\n",
      "\tTrain Loss: 0.053 | Train PPL:   1.054\n",
      "\t Val. Loss: 1.862 |  Val. PPL:   6.435 | Avg Bleu: 0.817815\n",
      "Epoch: 37 | Time: 0m 32s\n",
      "\tTrain Loss: 0.053 | Train PPL:   1.054\n",
      "\t Val. Loss: 2.009 |  Val. PPL:   7.452 | Avg Bleu: 0.814421\n",
      "Epoch: 38 | Time: 0m 31s\n",
      "\tTrain Loss: 0.049 | Train PPL:   1.051\n",
      "\t Val. Loss: 2.065 |  Val. PPL:   7.884 | Avg Bleu: 0.818449\n",
      "Epoch: 39 | Time: 0m 30s\n",
      "\tTrain Loss: 0.048 | Train PPL:   1.049\n",
      "\t Val. Loss: 1.897 |  Val. PPL:   6.667 | Avg Bleu: 0.821087\n",
      "Epoch: 40 | Time: 0m 30s\n",
      "\tTrain Loss: 0.051 | Train PPL:   1.052\n",
      "\t Val. Loss: 2.096 |  Val. PPL:   8.136 | Avg Bleu: 0.823436\n",
      "Epoch: 41 | Time: 0m 30s\n",
      "\tTrain Loss: 0.049 | Train PPL:   1.050\n",
      "\t Val. Loss: 2.281 |  Val. PPL:   9.788 | Avg Bleu: 0.825080\n",
      "Epoch: 42 | Time: 0m 30s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 2.210 |  Val. PPL:   9.114 | Avg Bleu: 0.827703\n",
      "Epoch: 43 | Time: 0m 30s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 2.246 |  Val. PPL:   9.449 | Avg Bleu: 0.827555\n",
      "Epoch: 44 | Time: 0m 30s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 2.392 |  Val. PPL:  10.939 | Avg Bleu: 0.828829\n",
      "Epoch: 45 | Time: 0m 30s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 2.527 |  Val. PPL:  12.513 | Avg Bleu: 0.828797\n",
      "Epoch: 46 | Time: 0m 30s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 2.928 |  Val. PPL:  18.693 | Avg Bleu: 0.829496\n",
      "Epoch: 47 | Time: 0m 30s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 3.012 |  Val. PPL:  20.330 | Avg Bleu: 0.830172\n",
      "Epoch: 48 | Time: 0m 30s\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.220\n",
      "\t Val. Loss: 3.095 |  Val. PPL:  22.082 | Avg Bleu: 0.830009\n",
      "Epoch: 49 | Time: 0m 30s\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.205\n",
      "\t Val. Loss: 3.050 |  Val. PPL:  21.107 | Avg Bleu: 0.828715\n",
      "Epoch: 50 | Time: 0m 30s\n",
      "\tTrain Loss: 0.998 | Train PPL:   2.713\n",
      "\t Val. Loss: 3.282 |  Val. PPL:  26.621 | Avg Bleu: 0.830344\n",
      "Epoch: 51 | Time: 0m 30s\n",
      "\tTrain Loss: 363.361 | Train PPL: 63911656270326566637405131907537919774462409834012792427916708614950753362524158060567070345419844809858353145043072949222815246707924734148223848032946356224.000\n",
      "\t Val. Loss: 108.204 |  Val. PPL: 98264441613912696498926847275242531382657810432.000 | Avg Bleu: 0.014126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-8:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Andriy\\Anaconda3\\lib\\threading.py\", line 917, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\Andriy\\Anaconda3\\lib\\site-packages\\tensorboardX\\event_file_writer.py\", line 180, in run\n",
      "    self._ev_writer.write_event(event)\n",
      "  File \"C:\\Users\\Andriy\\Anaconda3\\lib\\site-packages\\tensorboardX\\event_file_writer.py\", line 61, in write_event\n",
      "    return self._write_serialized_event(event.SerializeToString())\n",
      "  File \"C:\\Users\\Andriy\\AppData\\Roaming\\Python\\Python37\\site-packages\\google\\protobuf\\internal\\python_message.py\", line 1042, in SerializeToString\n",
      "    return self.SerializePartialToString(**kwargs)\n",
      "  File \"C:\\Users\\Andriy\\AppData\\Roaming\\Python\\Python37\\site-packages\\google\\protobuf\\internal\\python_message.py\", line 1051, in SerializePartialToString\n",
      "    self._InternalSerialize(out.write, **kwargs)\n",
      "  File \"C:\\Users\\Andriy\\AppData\\Roaming\\Python\\Python37\\site-packages\\google\\protobuf\\internal\\python_message.py\", line 1071, in InternalSerialize\n",
      "    field_descriptor._encoder(write_bytes, field_value, deterministic)\n",
      "  File \"C:\\Users\\Andriy\\AppData\\Roaming\\Python\\Python37\\site-packages\\google\\protobuf\\internal\\encoder.py\", line 767, in EncodeField\n",
      "    return value._InternalSerialize(write, deterministic)\n",
      "  File \"C:\\Users\\Andriy\\AppData\\Roaming\\Python\\Python37\\site-packages\\google\\protobuf\\internal\\python_message.py\", line 1071, in InternalSerialize\n",
      "    field_descriptor._encoder(write_bytes, field_value, deterministic)\n",
      "  File \"C:\\Users\\Andriy\\AppData\\Roaming\\Python\\Python37\\site-packages\\google\\protobuf\\internal\\encoder.py\", line 761, in EncodeRepeatedField\n",
      "    element._InternalSerialize(write, deterministic)\n",
      "  File \"C:\\Users\\Andriy\\AppData\\Roaming\\Python\\Python37\\site-packages\\google\\protobuf\\internal\\python_message.py\", line 1071, in InternalSerialize\n",
      "    field_descriptor._encoder(write_bytes, field_value, deterministic)\n",
      "  File \"C:\\Users\\Andriy\\AppData\\Roaming\\Python\\Python37\\site-packages\\google\\protobuf\\internal\\encoder.py\", line 611, in EncodeField\n",
      "    write(local_struct_pack(format, value))\n",
      "OverflowError: float too large to pack with f format\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 52 | Time: 0m 30s\n"
     ]
    },
    {
     "ename": "OverflowError",
     "evalue": "math range error",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-672ccc06451d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Epoch: {epoch:02} | Time: {epoch_mins}m {epoch_secs}s'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f} | Avg Bleu: {avg_bleu:.6f}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mPLOT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOverflowError\u001b[0m: math range error"
     ]
    }
   ],
   "source": [
    "from itertools import count\n",
    "\n",
    "N_EPOCHS = 100\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "PLOT = True # Set to false to disable plotting to tensorboard\n",
    "writer = SummaryWriter(comment='_cnn')\n",
    "\n",
    "for epoch in count(1):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_loader, optimizer, criterion, CLIP)\n",
    "    valid_loss, avg_bleu = evaluate(model, test_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut5-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f} | Avg Bleu: {avg_bleu:.6f}')\n",
    "    if PLOT:\n",
    "        writer.add_scalar('train/epoch_sec', epoch_mins * 60 + epoch_secs, epoch)\n",
    "        writer.add_scalar('train/loss', train_loss, epoch)\n",
    "        writer.add_scalar('train/ppl', math.exp(train_loss), epoch)\n",
    "        writer.add_scalar('valid/loss', valid_loss, epoch)\n",
    "        writer.add_scalar('valid/ppl', math.exp(valid_loss), epoch)\n",
    "        writer.add_scalar('valid/avg_bleu', avg_bleu, epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
